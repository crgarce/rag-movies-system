# Microservicio: Movies Retrieval Generator

Este microservicio es responsable de procesar un archivo CSV con informaciÃ³n de pelÃ­culas, generar embeddings a partir de las descripciones utilizando la API de OpenAI, e indexarlos en una base de datos PostgreSQL con soporte para vectores gracias a PGVector.

---

## Estructura del Proyecto
El proyecto sigue los principios de **arquitectura limpia** para garantizar modularidad y escalabilidad. A continuaciÃ³n, la estructura principal:

```plaintext
# Microservicio `retrieval`

El microservicio `retrieval` es una parte fundamental del sistema **RAG (Retrieval-Augmented Generation)**. Su objetivo es recibir preguntas del usuario, buscar informaciÃ³n relevante en la base de conocimiento previamente indexada, y generar respuestas claras utilizando la API de OpenAI.

---

## **Estructura del Proyecto**

La estructura del proyecto sigue principios de arquitectura limpia para garantizar modularidad, mantenibilidad y facilidad de pruebas.

```plaintext
/retrieval
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ application/
â”‚   â”‚   â””â”€â”€ usecases/
â”‚   â”‚       â””â”€â”€ question_answer.py  # Caso de uso principal
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ middleware/
â”‚   â”‚   â”‚   â””â”€â”€ validate_consumer.py  # Middleware para validar Consumer-ID
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”‚   â”œâ”€â”€ models.py  # Modelos del dominio
â”‚   â”‚   â”‚   â””â”€â”€ requests.py  # Modelos de entrada
â”‚   â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”‚   â”œâ”€â”€ config.py  # ConfiguraciÃ³n global
â”‚   â”‚   â”‚   â””â”€â”€ logger.py  # Logs avanzados
â”‚   â”œâ”€â”€ infra/
â”‚   â”‚   â”œâ”€â”€ repositories/
â”‚   â”‚   â”‚   â””â”€â”€ pgvector_repository.py  # Repositorio para interacciÃ³n con la base de datos
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â”œâ”€â”€ embedding_service.py  # GeneraciÃ³n de embeddings
â”‚   â”‚   â”‚   â””â”€â”€ response_generator.py  # GeneraciÃ³n de respuestas
â”‚   â”‚   â””â”€â”€ container.py  # InyecciÃ³n de dependencias
â”‚   â””â”€â”€ main.py  # Punto de entrada del microservicio
â”œâ”€â”€ Dockerfile  # ConfiguraciÃ³n de la imagen Docker
â”œâ”€â”€ docker-compose.yml  # Levantar el servicio en local
â”œâ”€â”€ requirements.txt  # Dependencias del proyecto
â””â”€â”€ .env  # Variables de entorno (sensibles)
```

## **Dependencias**
Estas son las principales dependencias del microservicio y sus propÃ³sitos:

- **FastAPI**: Framework web para manejar los endpoints.
- **Uvicorn**: Servidor ASGI para ejecutar la aplicaciÃ³n.
- **OpenAI**: Cliente para interactuar con la API de OpenAI.
- **Psycopg2**: Conector para PostgreSQL.
- **Numpy**: Para cÃ¡lculos numÃ©ricos necesarios en embeddings.
- **Python-dotenv**: Manejo de variables de entorno.
- **Tenacity**: Reintentos automÃ¡ticos para llamadas a OpenAI.

Consulta el archivo `requirements.txt` para ver las versiones especÃ­ficas.

---

## **Archivo `.env`**
El archivo `.env` contiene configuraciones sensibles y especÃ­ficas del entorno. AsegÃºrate de crearlo en la raÃ­z del proyecto.

**Ejemplo de contenido de `.env`:**
```env
DB_HOST=postgres-db
DB_PORT=5432
DB_NAME=rag_db
DB_USER=ragadmin
DB_PASSWORD=<tu-contraseÃ±a>
OPENAI_API_KEY=<tu-api-key-de-openai>
EMBEDDING_MODEL=text-embedding-ada-002
CHAT_COMPLETION_MODEL=gpt-3.5-turbo
CONSUMER_ID=<tu-consumer-id>
```

---

## **CÃ³mo consumir el servicio**

El endpoint principal del microservicio es:

### **POST** `/question-answer/`
Recibe una pregunta del usuario y devuelve una respuesta generada.

#### **Headers**
```plaintext
x-consumer-id: <tu-consumer-id>
Content-Type: application/json
```

#### **Body (JSON)**
```json
{
  "question": "What is The Matrix?"
}
```

#### **Respuesta esperada**
```json
{
  "status_code": 200,
  "message": "Respuesta generada con Ã©xito.",
  "data": "Matrix es una pelÃ­cula de ciencia ficciÃ³n lanzada en 1999..."
}
```

---

## **CÃ³mo levantar el microservicio en local**

### **1. Clonar el repositorio**
```bash
git clone <tu-repositorio>
cd retrieval
```

### **2. Crear y activar un entorno virtual (en linux)**
```bash
python3 -m venv venv
source venv/bin/activate
```

### **3. Instalar dependencias**
```bash
pip install -r requirements.txt
```

### **4. Crear el archivo `.env`**
Configura las variables necesarias como se muestra en el ejemplo.

### **5. Levantar el servicio**
```bash
uvicorn app.main:app --host 0.0.0.0 --port 8001
```

---

## **CÃ³mo levantarlo con Docker**

### **1. Construir la imagen Docker**
Desde el directorio del microservicio, ejecuta:
```bash
docker build -t retrieval-service .
```

### **2. Levantar el contenedor**
```bash
docker-compose up --build
```

Esto levantarÃ¡ el microservicio en el puerto **8001**.
IMPORTANTE: Para levantar el microservicio, la Base de Datos debe estar arriba tambien.

---

## **ExplicaciÃ³n del flujo**

1. **RecepciÃ³n de la pregunta:**
   - El usuario envÃ­a una pregunta al endpoint `/question-answer/`.
   - Se valida el `x-consumer-id` a travÃ©s del middleware.

2. **GeneraciÃ³n del embedding:**
   - La pregunta se convierte en un embedding utilizando la API de OpenAI.

3. **Consulta en la base de conocimiento:**
   - Se utiliza el embedding generado para buscar pelÃ­culas relevantes en PostgreSQL con PGVector.

4. **GeneraciÃ³n de la respuesta:**
   - Con base en los resultados, se genera una respuesta utilizando la API de Chat Completion de OpenAI.

5. **DevoluciÃ³n al usuario:**
   - Se retorna la respuesta generada al usuario en un formato JSON.

---

## **Contacto**
Si tienes alguna duda o problema, no dudes en abrir un issue en el repositorio.

Â¡Gracias por usar el microservicio `retrieval`! ðŸš€
```

Puedes copiar y pegar este contenido directamente en un archivo llamado `README.md` en el directorio del microservicio. Si necesitas algo mÃ¡s, Â¡avÃ­same! ðŸš€